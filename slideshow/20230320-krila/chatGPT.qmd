
# ê±°ëŒ€ ì–¸ì–´ëª¨í˜•

## chatGPT ë€?

::: panel-tabset

### PNGì™€ JPEG

![](../../img/png_jpg.png){width="651"}

::: aside
ìë£Œì¶œì²˜: [Ted Chiang (February 9, 2023), "ChatGPT Is a Blurry JPEG of the Web - OpenAI's chatbot offers paraphrases, whereas Google offers quotes. Which do we prefer?", The New Yorker](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web)
:::

### Foundation ëª¨í˜•

![](../../img/foundation_model.png){width="500"}
:::


## ê±°ëŒ€ ì–¸ì–´ëª¨í˜•(LLM) 


:::{.panel-tabset}

## LLM ì§„í™”

![](../../img/LLM_tree.gif)

## 80ì–µ íŒ¨ëŸ¬ë¯¸í„°

![](../../img/LLM_tree_8_billion.png)

## 400ì–µ 

![](../../img/LLM_tree_40_billion.png)

## 640ì–µ 

![](../../img/LLM_tree_62_billion.png)


## 5,400ì–µ 

![](../../img/LLM_tree_540_billion.png)

## ì„±ëŠ¥

![](../../img/LLM_tree_performance.png)

:::


::: aside
[[Sharan Narang and Aakanksha Chowdhery (APRIL 04, 2022), "Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance", Software Engineers, Google Research](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)]{.aside}
:::


## ì£¼ìš” AI ëª¨í˜• ê°œë°œë¹„

ì–¸ì–´ ëª¨í˜• ê°œë°œì€ 2010ë…„ ì´í›„ ê°œë°œë¹„ìš©ì´ ê¸‰ê²©íˆ ì¦ê°€í•˜ê³  ìˆìŒ.

<iframe src="https://ourworldindata.org/grapher/artificial-intelligence-training-computation?yScale=linear&time=2017-06-12..latest" loading="lazy" style="width: 100%; height: 500px; border: 0px none;"></iframe>

::: aside
[Estimating ğŸŒ´PaLM's training cost](https://blog.heim.xyz/palm-training-cost/)
:::


##  {background-image="../../img/question.jpg"}

::: r-fit-text
[ì „í†µ, ë””ì§€í„¸, AI ì§€í‘œ &rarr; ì§€ì›ì?]{style="color: red; font-size:77px;"}
:::

