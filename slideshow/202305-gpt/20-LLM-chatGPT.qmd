# `chatGPT` → 챗GPT → LLM

## 챗GPT 란?

::: panel-tabset
### PNG와 JPEG

![](img/png_jpg.png){width="651"}

::: aside
자료출처: [Ted Chiang (February 9, 2023), "ChatGPT Is a Blurry JPEG of the Web - OpenAI's chatbot offers paraphrases, whereas Google offers quotes. Which do we prefer?", The New Yorker](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web)
:::

### Foundation 모형

![](img/foundation_model.png){width="500"}
:::

## 거대 언어모형(LLM)

::: panel-tabset
### LLM 진화

![](img/LLM_tree.gif)

### 80억 패러미터

![](img/LLM_tree_8_billion.png)

### 400억

![](img/LLM_tree_40_billion.png)

### 640억

![](img/LLM_tree_62_billion.png)

### 5,400억

![](img/LLM_tree_540_billion.png)

### 성능

![](img/LLM_tree_performance.png)
:::

::: aside
[[Sharan Narang and Aakanksha Chowdhery (APRIL 04, 2022), "Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance", Software Engineers, Google Research](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)]{.aside}
:::

## 현재 나의 모습

![](img/now_me.png)

## LLM 진화

![](img/evolution_tree.jpg){width="650"}

::: aside
[@yang2023harnessing]
:::

## 공개형 GPT vs 폐쇄형 GPT

::: panel-tabset
### Stable Diffusion

![](img/Stable-Diffusion-Developer-Adoption-1-scaled.webp){width="700"}

### LLM

![](img/llm_g.png){width="700"}

### GPT4ALL

![](img/gpt4all-demo.jpg){width="700"}
:::

::: aside
-   데이터 출처: [@zhao2023survey]
-   [chatGPT - 오픈소스 S/W](https://r2bit.com/chatGPT/open_source.html)
:::

## LLM

::: panel-tabset
### 독점/상용 LLM

![](img/llm_proprietary.jpg)

### 오픈소스/공개 LLM

![](img/llm_open_source.jpg)
:::

::: aside
[@fullstack2023]
:::

## 토큰 크기 (Context Length) {.smaller}

::: panel-tabset
### 표

| 회사명    | LLM 모형           | Context 길이 (토큰 크기) | 텍스트 크기                |
|-----------------|-----------------|--------------------|--------------------|
| OpenAI    | GPT-1              | 512                      | 1 쪽, 4 단락, 36줄         |
| 구글      | PaLM               | 512                      | ↑                          |
| OpenAI    | GPT-2              | 1024                     | 2 쪽, 8 단락, 68줄         |
| OpenAI    | GPT-3              | 2048                     | 1500 영단어, 4쪽, 139줄    |
| OpenAI    | GPT-4              | 2048                     | ↑                          |
| Meta      | Llama              | 2048                     | ↑                          |
| OpenAI    | GPT-3.5-Turbo      | 4096                     | 9쪽, 369줄                 |
| 구글      | PaLM2              | 8192                     | 13쪽, 단편 소설, 기술 문서 |
| OpenAI    | GPT-4 8K           | 8192                     | ↑                          |
| OpenAI    | GPT-4 32K          | 32000                    | 48쪽, 대학학위논문         |
| MosaicML  | MPT-7B-StoryWriter | 65000                    | 98쪽                       |
| Anthropic | Claude             | 100000                   | 150쪽, 소설책 반권         |

### 그래프

![](img/FsUnTB9aAAAkBCc.jfif){width="642"}

--- Dan Fu @realDanFu <a href="https://twitter.com/realDanFu/status/1640762299408601089?s=20"> March 29, 2023</a>

:::

## HuggingGPT

::: panel-tabset
### LLM as Controller

![](img/huggingGPT.jpg)

### HuggingGPT 개요

![](img/huggingGPT-generate.jpg){width="500"}

### 사례

![](img/huggingGPT-example.jpg){width="500"}
:::

::: aside
출처: [@shen2023hugginggpt]
:::

